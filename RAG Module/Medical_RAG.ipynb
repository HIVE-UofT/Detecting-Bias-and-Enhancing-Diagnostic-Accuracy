{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TObuHk7rGJkg"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) Notebook\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline for answering user queries. It combines information retrieval using dense embeddings with text generation to generate accurate and contextually relevant answers based on its medical knowledge source (CSV with question-answer pairs).\n",
        "\n",
        "## Features\n",
        "- **Text Preprocessing**: Normalizes and cleans input data (questions) before processing.\n",
        "- **Knowledge Source Embedding**: Embeds questions and answers into vector space using Dense Passage Retrieval (DPR).\n",
        "- **Similarity Search**: Retrieves the top-k most relevant questions from the knowledge source based on user queries.\n",
        "- **Answer Generation**: Generates contextually relevant answers using a large language model (Gemma-2-2B).\n",
        "\n",
        "## Steps\n",
        "1. **Load Knowledge Source**: Load a CSV containing question-answer pairs.\n",
        "2. **Text Preprocessing**: Clean and normalize text before encoding.\n",
        "3. **Embedding and Indexing**: Use DPR encoder to embed questions and answers, then index them for fast retrieval.\n",
        "4. **Query Retrieval**: Given a user query, retrieve the top-k most relevant question-answer pairs.\n",
        "5. **Answer Generation**: Use the Gemma-2-2B model to generate an answer from the retrieved contexts.\n",
        "\n",
        "## Requirements\n",
        "- Python 3.x\n",
        "- PyTorch\n",
        "- HuggingFace Transformers\n",
        "- FAISS\n",
        "- Pandas\n",
        "\n",
        "## How to Use\n",
        "1. Place your question-answer CSV in the same directory as the notebook, or specify the path in `knowledge_source_path`.\n",
        "2. Run the notebook to query and retrieve answers from the knowledge source.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-25T12:35:27.406464Z",
          "iopub.status.busy": "2024-12-25T12:35:27.406130Z",
          "iopub.status.idle": "2024-12-25T12:35:27.410230Z",
          "shell.execute_reply": "2024-12-25T12:35:27.409496Z",
          "shell.execute_reply.started": "2024-12-25T12:35:27.406428Z"
        },
        "id": "zSpkQ_aNFjSB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-25T12:49:41.415916Z",
          "iopub.status.busy": "2024-12-25T12:49:41.415496Z",
          "iopub.status.idle": "2024-12-25T12:49:41.461930Z",
          "shell.execute_reply": "2024-12-25T12:49:41.461092Z",
          "shell.execute_reply.started": "2024-12-25T12:49:41.415884Z"
        },
        "id": "SYg9yZz_FjSD",
        "outputId": "6039d2cc-4871-4c41-a5a9-46d0a9aab06a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-25T12:35:03.931020Z",
          "iopub.status.busy": "2024-12-25T12:35:03.930755Z",
          "iopub.status.idle": "2024-12-25T12:35:16.066184Z",
          "shell.execute_reply": "2024-12-25T12:35:16.065338Z",
          "shell.execute_reply.started": "2024-12-25T12:35:03.931000Z"
        },
        "id": "XmEDjxfNFjSD",
        "outputId": "c9702dcd-8729-4e98-95bc-e4460ebea031",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu, sentence-transformers\n",
            "Successfully installed faiss-cpu-1.9.0.post1 sentence-transformers-3.3.1\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas transformers faiss-cpu sentence-transformers\n",
        "!pip install openpyxl\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-25T12:43:18.018211Z",
          "iopub.status.busy": "2024-12-25T12:43:18.017902Z",
          "iopub.status.idle": "2024-12-25T12:43:42.464855Z",
          "shell.execute_reply": "2024-12-25T12:43:42.463959Z",
          "shell.execute_reply.started": "2024-12-25T12:43:18.018184Z"
        },
        "id": "VJCOIIblFjSG",
        "outputId": "323abcf8-9a25-48a3-c702-7eb70886be63",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: A patient is presenting with the following symptoms: Back pain, Ache all over, Neck pain. Based on these symptoms, which of the following diseases is the most likely diagnosis?\n",
            "1) Fibromyalgia\n",
            "2) Spondylitis\n",
            "3) Polycystic ovarian syndrome (PCOS)\n",
            "4) Breast infection (mastitis)\n",
            "\n",
            "Diseases Identified: ['Fibromyalgia', 'Spondylitis', 'Polycystic ovarian syndrome (PCOS)', 'Breast infection (mastitis)']\n",
            "\n",
            "Generated Sub-Queries: ['What are the symptoms of Fibromyalgia?', 'What are the symptoms of Spondylitis?', 'What are the symptoms of Polycystic ovarian syndrome (PCOS)?', 'What are the symptoms of Breast infection (mastitis)?']\n",
            "\n",
            "Retrieved Information for Each Disease:\n",
            "\n",
            "Sub-Query: What are the symptoms of Fibromyalgia?\n",
            "  1. Question: what is fibromyalgia?, Answer: Fibromyalgia (FM or FMS) is characterised by chronic widespread pain and allodynia (a heightened and painful response to pressure). Its exact cause is unknown but is believed to involve psychological,...\n",
            "  2. Question: what are the symptoms of fibromyalgia?, Answer: Back pain, Ache all over, Neck pain, Muscle pain, Leg pain, Headache, Shoulder pain, Low back pain, Sharp chest pain, Arm pain, Hip pain, Fatigue...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Sub-Query: What are the symptoms of Spondylitis?\n",
            "  1. Question: what is spondylitis?, Answer: Spondylitis is an inflammation of the vertebra. It is a form of spondylopathy. In many cases spondylitis involves one or more vertebral joints as well, which itself is called spondylarthritis. ...\n",
            "  2. Question: what are the symptoms of spondylitis?, Answer: Back pain, Low back pain, Leg pain, Neck pain, Hip pain, Side pain, Back cramps or spasms, Lower body pain, Stiffness all over, Muscle cramps, contractures, or spasms, Low back cramps or spasms, Back ...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Sub-Query: What are the symptoms of Polycystic ovarian syndrome (PCOS)?\n",
            "  1. Question: what is polycystic ovarian syndrome (pcos)?, Answer: A complex disorder characterized by infertility, HIRSUTISM; OBESITY; and various menstrual disturbances such as OLIGOMENORRHEA; AMENORRHEA; ANOVULATION. Polycystic ovary syndrome is usually associated...\n",
            "  2. Question: what are the symptoms of polycystic ovarian syndrome (pcos)?, Answer: Infertility, Unpredictable menstruation, Weight gain, Pelvic pain, Absence of menstruation, Heavy menstrual flow, Intermenstrual bleeding, Long menstrual periods, Painful menstruation, Cramps and spas...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Sub-Query: What are the symptoms of Breast infection (mastitis)?\n",
            "  1. Question: what are the symptoms of breast infection (mastitis)?, Answer: Pain or soreness of breast, Lump or mass of breast, Abnormal appearing skin, Fever, Bleeding or discharge from nipple, Postpartum problems of the breast, Recent pregnancy, Chills, Bones are painful, S...\n",
            "  2. Question: what is breast infection (mastitis)?, Answer: Mastitis is the inflammation of breast tissue. S. aureus is the most common etiological organism responsible, but S. epidermidis and streptococci are occasionally isolated as well. ...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import (\n",
        "    DPRContextEncoder,\n",
        "    DPRQuestionEncoder,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoderTokenizer\n",
        ")\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Load the knowledge source\n",
        "def load_knowledge_source(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    questions_answers = []\n",
        "    for _, row in df.iterrows():\n",
        "        questions_answers.append({\n",
        "            \"Question\": preprocess_text(row['Question']),  # Apply preprocessing\n",
        "            \"Answer\": row['Answer']\n",
        "        })\n",
        "    return questions_answers\n",
        "\n",
        "# Embed and index the knowledge source\n",
        "def embed_and_index_knowledge(questions_answers):\n",
        "    context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    embeddings = []\n",
        "    max_length = 256\n",
        "\n",
        "    for qa in questions_answers:\n",
        "        text = preprocess_text(f\"Question: {qa['Question']}\")  # Only include the question\n",
        "        inputs = context_tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = context_encoder(**inputs).pooler_output\n",
        "            normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
        "            embeddings.append(normalized_embedding.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)  # Ensure Inner Product similarity\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return context_encoder, context_tokenizer, index\n",
        "\n",
        "# Retrieve the top-k results\n",
        "def retrieve_top_k(question_encoder, question_tokenizer, index, query, questions_answers, k=2):\n",
        "    query = preprocess_text(query)  # Preprocess query\n",
        "\n",
        "    # Tokenize the query\n",
        "    inputs = question_tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "        normalized_query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1).cpu().numpy()\n",
        "\n",
        "    # Retrieve the top k most relevant questions\n",
        "    distances, indices = index.search(normalized_query_embedding, k)\n",
        "\n",
        "    # Get the corresponding questions and answers\n",
        "    retrieved_data = [\n",
        "        {\"Question\": questions_answers[i]['Question'], \"Answer\": questions_answers[i]['Answer']}\n",
        "        for i in indices[0]\n",
        "    ]\n",
        "    return retrieved_data, distances[0]\n",
        "\n",
        "\n",
        "def extract_diseases_from_query(query):\n",
        "    \"\"\"Extract disease options from the query.\"\"\"\n",
        "    # Match lines starting with numbers followed by ')'\n",
        "    pattern = r\"\\d\\)\\s([^\\n]+)\"\n",
        "    diseases = re.findall(pattern, query)\n",
        "    return diseases\n",
        "\n",
        "def generate_sub_queries(diseases):\n",
        "    \"\"\"Generate sub-queries for each disease.\"\"\"\n",
        "    sub_queries = [f\"What are the symptoms of {disease}?\" for disease in diseases]\n",
        "    return sub_queries\n",
        "\n",
        "def retrieve_for_sub_queries(question_encoder, question_tokenizer, index, sub_queries, questions_answers):\n",
        "    \"\"\"Retrieve answers for each sub-query.\"\"\"\n",
        "    results = {}\n",
        "    for sub_query in sub_queries:\n",
        "        retrieved_data, distances = retrieve_top_k(\n",
        "            question_encoder,\n",
        "            question_tokenizer,\n",
        "            index,\n",
        "            sub_query,\n",
        "            questions_answers\n",
        "        )\n",
        "        results[sub_query] = retrieved_data\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    # Load the question encoder model and tokenizer\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(\n",
        "        \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "    ).to(device)\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
        "        \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "    )\n",
        "\n",
        "    # Load the knowledge source (CSV with question-answer pairs)\n",
        "    knowledge_source_path = \"knowledge-source.csv\"\n",
        "    questions_answers = load_knowledge_source(knowledge_source_path)\n",
        "\n",
        "    # Embed and index the questions\n",
        "    context_encoder, context_tokenizer, index = embed_and_index_knowledge(questions_answers)\n",
        "\n",
        "    # Example query\n",
        "    query = \"\"\"A patient is presenting with the following symptoms: Back pain, Ache all over, Neck pain. Based on these symptoms, which of the following diseases is the most likely diagnosis?\n",
        "1) Fibromyalgia\n",
        "2) Spondylitis\n",
        "3) Polycystic ovarian syndrome (PCOS)\n",
        "4) Breast infection (mastitis)\"\"\"\n",
        "\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "\n",
        "    # Extract diseases from the query\n",
        "    diseases = extract_diseases_from_query(query)\n",
        "    print(f\"\\nDiseases Identified: {diseases}\")\n",
        "\n",
        "    # Generate sub-queries\n",
        "    sub_queries = generate_sub_queries(diseases)\n",
        "    print(f\"\\nGenerated Sub-Queries: {sub_queries}\")\n",
        "\n",
        "    # Retrieve information for each sub-query\n",
        "    retrieved_results = retrieve_for_sub_queries(\n",
        "        question_encoder,\n",
        "        question_tokenizer,\n",
        "        index,\n",
        "        sub_queries,\n",
        "        questions_answers\n",
        "    )\n",
        "\n",
        "    # Display the results\n",
        "    print(\"\\nRetrieved Information for Each Disease:\")\n",
        "    for sub_query, data in retrieved_results.items():\n",
        "        print(f\"\\nSub-Query: {sub_query}\")\n",
        "        for i, retrieved in enumerate(data, 1):\n",
        "            print(f\"  {i}. Question: {retrieved['Question']}, Answer: {retrieved['Answer'][:200]}...\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "929b6df398084700becf1e96a0bce6b0"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-25T13:00:47.351433Z",
          "iopub.status.busy": "2024-12-25T13:00:47.351128Z",
          "iopub.status.idle": "2024-12-25T13:01:27.025717Z",
          "shell.execute_reply": "2024-12-25T13:01:27.024849Z",
          "shell.execute_reply.started": "2024-12-25T13:00:47.351409Z"
        },
        "id": "PntlmTmEFjSH",
        "outputId": "cc516082-961b-45f8-e411-b7befdcc5e4f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "929b6df398084700becf1e96a0bce6b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: A patient is presenting with the following symptoms: Back pain, Ache all over, Neck pain. Based on these symptoms, which of the following diseases is the most likely diagnosis?\n",
            "1) Fibromyalgia\n",
            "2) Chalazion\n",
            "3) Polycystic ovarian syndrome (PCOS)\n",
            "4) Breast infection (mastitis)\n",
            "\n",
            "Answer: **1) Fibromyalgia**\n",
            "\n",
            "Explanation:**\n",
            "The provided text mentions Fibromyalgia as a condition characterized by widespread pain, and allodynia (pain response to pressure). The patient's symptoms of back pain, ache all over, and neck pain are all consistent with the description of fibromyalgia. \n",
            "\n",
            "\n",
            "Let me know if you have other questions.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import (\n",
        "    DPRContextEncoder,\n",
        "    DPRQuestionEncoder,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Load the knowledge source\n",
        "def load_knowledge_source(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    questions_answers = []\n",
        "    for _, row in df.iterrows():\n",
        "        questions_answers.append({\n",
        "            \"Question\": preprocess_text(row['Question']),  # Apply preprocessing\n",
        "            \"Answer\": row['Answer']\n",
        "        })\n",
        "    return questions_answers\n",
        "\n",
        "# Embed and index the knowledge source\n",
        "def embed_and_index_knowledge(questions_answers):\n",
        "    context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    embeddings = []\n",
        "    max_length = 256\n",
        "\n",
        "    for qa in questions_answers:\n",
        "        text = preprocess_text(f\"Question: {qa['Question']}\")  # Only include the question\n",
        "        inputs = context_tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = context_encoder(**inputs).pooler_output\n",
        "            normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
        "            embeddings.append(normalized_embedding.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)  # Ensure Inner Product similarity\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return context_encoder, context_tokenizer, index\n",
        "\n",
        "# Retrieve the top-k results\n",
        "def retrieve_top_k(question_encoder, question_tokenizer, index, query, questions_answers, k=2):\n",
        "    query = preprocess_text(query)  # Preprocess query\n",
        "\n",
        "    # Tokenize the query\n",
        "    inputs = question_tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "        normalized_query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1).cpu().numpy()\n",
        "\n",
        "    # Retrieve the top k most relevant questions\n",
        "    distances, indices = index.search(normalized_query_embedding, k)\n",
        "\n",
        "    # Get the corresponding questions and answers\n",
        "    retrieved_data = [\n",
        "        {\"Question\": questions_answers[i]['Question'], \"Answer\": questions_answers[i]['Answer']}\n",
        "        for i in indices[0]\n",
        "    ]\n",
        "    return retrieved_data, distances[0]\n",
        "\n",
        "# Extract diseases from query\n",
        "def extract_diseases_from_query(query):\n",
        "    pattern = r\"\\d\\)\\s([^\\n]+)\"\n",
        "    diseases = re.findall(pattern, query)\n",
        "    return diseases\n",
        "\n",
        "# Generate sub-queries for diseases\n",
        "def generate_sub_queries(diseases):\n",
        "    sub_queries = [f\"What are the symptoms of {disease}?\" for disease in diseases]\n",
        "    return sub_queries\n",
        "\n",
        "# Retrieve information for sub-queries\n",
        "def retrieve_for_sub_queries(question_encoder, question_tokenizer, index, sub_queries, questions_answers):\n",
        "    results = {}\n",
        "    for sub_query in sub_queries:\n",
        "        retrieved_data, distances = retrieve_top_k(\n",
        "            question_encoder,\n",
        "            question_tokenizer,\n",
        "            index,\n",
        "            sub_query,\n",
        "            questions_answers\n",
        "        )\n",
        "        results[sub_query] = retrieved_data\n",
        "    return results\n",
        "\n",
        "# Generate an answer based on retrieved contexts\n",
        "def generate_answer_with_model(main_query, retrieved_results):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token='')\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", token='').to(device)\n",
        "\n",
        "    # Combine retrieved contexts\n",
        "    context_summary = \"\\n\".join([\n",
        "        f\"{i+1}. {retrieved['Answer']}\" for i, retrieved in enumerate(sum(retrieved_results.values(), []))\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Below are some relevant documents that might help answer the question.\n",
        "If the answer can be found in these documents, please provide it.\n",
        "\n",
        "Retrieved Documents:\n",
        "{context_summary}\n",
        "\n",
        "Question: {main_query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids[\"input_ids\"],\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer_parts = generated_text.split(\"Answer:\")\n",
        "    if len(answer_parts) > 1:\n",
        "        return answer_parts[-1].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "    knowledge_source_path = \"knowledge-source.csv\"\n",
        "    questions_answers = load_knowledge_source(knowledge_source_path)\n",
        "    context_encoder, context_tokenizer, index = embed_and_index_knowledge(questions_answers)\n",
        "\n",
        "    query = \"\"\"A patient is presenting with the following symptoms: Back pain, Ache all over, Neck pain. Based on these symptoms, which of the following diseases is the most likely diagnosis?\n",
        "1) Fibromyalgia\n",
        "2) Chalazion\n",
        "3) Polycystic ovarian syndrome (PCOS)\n",
        "4) Breast infection (mastitis)\"\"\"\n",
        "\n",
        "    diseases = extract_diseases_from_query(query)\n",
        "    sub_queries = generate_sub_queries(diseases)\n",
        "    retrieved_results = retrieve_for_sub_queries(\n",
        "        question_encoder,\n",
        "        question_tokenizer,\n",
        "        index,\n",
        "        sub_queries,\n",
        "        questions_answers\n",
        "    )\n",
        "\n",
        "    answer = generate_answer_with_model(query, retrieved_results)\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"\\nAnswer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a5fdd1bc3cf24369bb78b168827ca933"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-25T13:05:42.514903Z",
          "iopub.status.busy": "2024-12-25T13:05:42.514568Z",
          "iopub.status.idle": "2024-12-25T13:06:26.236744Z",
          "shell.execute_reply": "2024-12-25T13:06:26.235962Z",
          "shell.execute_reply.started": "2024-12-25T13:05:42.514878Z"
        },
        "id": "XERoiuAQFjSI",
        "outputId": "56cac16b-3288-4334-b21c-72be0a72aebb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5fdd1bc3cf24369bb78b168827ca933",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: A patient is presenting with the following symptoms: Shortness of breath, Sharp chest pain, Palpitations, Dizziness. Based on these symptoms, which of the following diseases is the most likely diagnosis?\n",
            "1) Astigmatism\n",
            "2) Pinworm infection\n",
            "3) Cerebral palsy\n",
            "4) Atrial fibrillation \n",
            "\n",
            "Answer: **4) Atrial fibrillation** \n",
            "\n",
            "Explanation:** \n",
            "\n",
            "The provided symptoms of shortness of breath, sharp chest pain, palpitations, and dizziness are all consistent with atrial fibrillation. \n",
            "\n",
            "Here's why:\n",
            "\n",
            "* **Shortness of breath:** This can be a sign of fluid buildup in the lungs, a potential complication of atrial fibrillation.\n",
            "* **Sharp chest pain:**  Could be caused by the heart struggling to pump effectively due to atrial fibrillation. \n",
            "* **Palpitations:**  A common symptom of atrial fibrillation due to the irregular heart rhythm.\n",
            "* **Dizziness:** Could be caused by the rapid, irregular heartbeat and blood flow changes. \n",
            "\n",
            "\n",
            "Let me know if you have any other questions.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import (\n",
        "    DPRContextEncoder,\n",
        "    DPRQuestionEncoder,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Load the knowledge source\n",
        "def load_knowledge_source(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    questions_answers = []\n",
        "    for _, row in df.iterrows():\n",
        "        questions_answers.append({\n",
        "            \"Question\": preprocess_text(row['Question']),  # Apply preprocessing\n",
        "            \"Answer\": row['Answer']\n",
        "        })\n",
        "    return questions_answers\n",
        "\n",
        "# Embed and index the knowledge source\n",
        "def embed_and_index_knowledge(questions_answers):\n",
        "    context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    embeddings = []\n",
        "    max_length = 256\n",
        "\n",
        "    for qa in questions_answers:\n",
        "        text = preprocess_text(f\"Question: {qa['Question']}\")  # Only include the question\n",
        "        inputs = context_tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = context_encoder(**inputs).pooler_output\n",
        "            normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
        "            embeddings.append(normalized_embedding.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)  # Ensure Inner Product similarity\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return context_encoder, context_tokenizer, index\n",
        "\n",
        "# Retrieve the top-k results\n",
        "def retrieve_top_k(question_encoder, question_tokenizer, index, query, questions_answers, k=2):\n",
        "    query = preprocess_text(query)  # Preprocess query\n",
        "\n",
        "    # Tokenize the query\n",
        "    inputs = question_tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "        normalized_query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1).cpu().numpy()\n",
        "\n",
        "    # Retrieve the top k most relevant questions\n",
        "    distances, indices = index.search(normalized_query_embedding, k)\n",
        "\n",
        "    # Get the corresponding questions and answers\n",
        "    retrieved_data = [\n",
        "        {\"Question\": questions_answers[i]['Question'], \"Answer\": questions_answers[i]['Answer']}\n",
        "        for i in indices[0]\n",
        "    ]\n",
        "    return retrieved_data, distances[0]\n",
        "\n",
        "# Extract diseases from query\n",
        "def extract_diseases_from_query(query):\n",
        "    pattern = r\"\\d\\)\\s([^\\n]+)\"\n",
        "    diseases = re.findall(pattern, query)\n",
        "    return diseases\n",
        "\n",
        "# Generate sub-queries for diseases\n",
        "def generate_sub_queries(diseases):\n",
        "    sub_queries = [f\"What are the symptoms of {disease}?\" for disease in diseases]\n",
        "    return sub_queries\n",
        "\n",
        "# Retrieve information for sub-queries\n",
        "def retrieve_for_sub_queries(question_encoder, question_tokenizer, index, sub_queries, questions_answers):\n",
        "    results = {}\n",
        "    for sub_query in sub_queries:\n",
        "        retrieved_data, distances = retrieve_top_k(\n",
        "            question_encoder,\n",
        "            question_tokenizer,\n",
        "            index,\n",
        "            sub_query,\n",
        "            questions_answers\n",
        "        )\n",
        "        results[sub_query] = retrieved_data\n",
        "    return results\n",
        "\n",
        "# Generate an answer based on retrieved contexts\n",
        "def generate_answer_with_model(main_query, retrieved_results):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token='')\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", token='').to(device)\n",
        "\n",
        "    # Combine retrieved contexts\n",
        "    context_summary = \"\\n\".join([\n",
        "        f\"{i+1}. {retrieved['Answer']}\" for i, retrieved in enumerate(sum(retrieved_results.values(), []))\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Below are some relevant documents that might help answer the question.\n",
        "If the answer can be found in these documents, please provide it.\n",
        "\n",
        "Retrieved Documents:\n",
        "{context_summary}\n",
        "\n",
        "Question: {main_query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids[\"input_ids\"],\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer_parts = generated_text.split(\"Answer:\")\n",
        "    if len(answer_parts) > 1:\n",
        "        return answer_parts[-1].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "    knowledge_source_path = \"knowledge-source.csv\"\n",
        "    questions_answers = load_knowledge_source(knowledge_source_path)\n",
        "    context_encoder, context_tokenizer, index = embed_and_index_knowledge(questions_answers)\n",
        "\n",
        "    query = \"\"\"A patient is presenting with the following symptoms: Shortness of breath, Sharp chest pain, Palpitations, Dizziness. Based on these symptoms, which of the following diseases is the most likely diagnosis?\n",
        "1) Astigmatism\n",
        "2) Pinworm infection\n",
        "3) Cerebral palsy\n",
        "4) Atrial fibrillation \"\"\"\n",
        "\n",
        "    diseases = extract_diseases_from_query(query)\n",
        "    sub_queries = generate_sub_queries(diseases)\n",
        "    retrieved_results = retrieve_for_sub_queries(\n",
        "        question_encoder,\n",
        "        question_tokenizer,\n",
        "        index,\n",
        "        sub_queries,\n",
        "        questions_answers\n",
        "    )\n",
        "\n",
        "    answer = generate_answer_with_model(query, retrieved_results)\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"\\nAnswer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "15ab0423ecd44281aec4696730f18a4c"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-25T13:13:22.847392Z",
          "iopub.status.busy": "2024-12-25T13:13:22.847107Z",
          "iopub.status.idle": "2024-12-25T13:14:00.763886Z",
          "shell.execute_reply": "2024-12-25T13:14:00.762986Z",
          "shell.execute_reply.started": "2024-12-25T13:13:22.847371Z"
        },
        "id": "4_9Xz5AYFjSK",
        "outputId": "34c5d875-d3dc-47c6-f33e-aaa9625f1429",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15ab0423ecd44281aec4696730f18a4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: help me. what is Trichiasis?\n",
            "\n",
            "Generated Answer:\n",
            "Trichiasis is a medical term for abnormally positioned eyelashes that grow back toward the eye, touching the cornea or conjunctiva. This can be caused by infection, inflammation, autoimmune conditions, congenital defects, eyelid agenesis and trauma such as burns or eyelid injury. \n",
            "\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "The answer is directly provided in the document.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import (\n",
        "    DPRContextEncoder,\n",
        "    DPRQuestionEncoder,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Load the knowledge source\n",
        "def load_knowledge_source(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    questions_answers = []\n",
        "    for _, row in df.iterrows():\n",
        "        questions_answers.append({\n",
        "            \"Question\": preprocess_text(row['Question']),  # Apply preprocessing\n",
        "            \"Answer\": row['Answer']\n",
        "        })\n",
        "    return questions_answers\n",
        "\n",
        "# Embed and index the knowledge source\n",
        "def embed_and_index_knowledge(questions_answers):\n",
        "    context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    embeddings = []\n",
        "    max_length = 256\n",
        "\n",
        "    for qa in questions_answers:\n",
        "        text = preprocess_text(f\"Question: {qa['Question']}\")  # Only include the question\n",
        "        inputs = context_tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = context_encoder(**inputs).pooler_output\n",
        "            normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
        "            embeddings.append(normalized_embedding.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)  # Ensure Inner Product similarity\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return context_encoder, context_tokenizer, index\n",
        "\n",
        "# Retrieve the top-k results\n",
        "def retrieve_top_k(question_encoder, question_tokenizer, index, query, questions_answers, k=2):\n",
        "    query = preprocess_text(query)  # Preprocess query\n",
        "\n",
        "    # Tokenize the query\n",
        "    inputs = question_tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "        normalized_query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1).cpu().numpy()\n",
        "\n",
        "    # Retrieve the top k most relevant questions\n",
        "    distances, indices = index.search(normalized_query_embedding, k)\n",
        "\n",
        "    # Get the corresponding questions and answers\n",
        "    retrieved_data = [\n",
        "        {\"Question\": questions_answers[i]['Question'], \"Answer\": questions_answers[i]['Answer']}\n",
        "        for i in indices[0]\n",
        "    ]\n",
        "    return retrieved_data, distances[0]\n",
        "\n",
        "# Generate an answer using a language model\n",
        "def generate_answer_with_model(question, retrieved_contexts):\n",
        "    \"\"\"\n",
        "    Generate an answer based on the retrieved contexts and the question.\n",
        "\n",
        "    Args:\n",
        "        question (str): The original query.\n",
        "        retrieved_contexts (list): Retrieved documents to use for answering the question.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer.\n",
        "    \"\"\"\n",
        "    # Initialize the language model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token='')\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", token='').to(device)\n",
        "\n",
        "    # Construct the input prompt with retrieved contexts\n",
        "    prompt = f\"\"\"\n",
        "Below are some relevant documents that might help answer the question.\n",
        "If the answer can be found in these documents, please provide it.\n",
        "\n",
        "Retrieved Documents:\n",
        "1. {retrieved_contexts[0]['Answer']}\n",
        "2. {retrieved_contexts[1]['Answer']}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "    # Tokenize the input prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    # Generate the answer\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids[\"input_ids\"],\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the generated answer (after \"Answer:\")\n",
        "    answer_parts = generated_text.split(\"Answer:\")\n",
        "    if len(answer_parts) > 1:\n",
        "        return answer_parts[-1].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "def main():\n",
        "    # Load the question encoder model and tokenizer\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(\n",
        "        \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "    ).to(device)\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
        "        \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "    )\n",
        "\n",
        "    # Load the knowledge source (CSV with question-answer pairs)\n",
        "    knowledge_source_path = \"knowledge-source.csv\"\n",
        "    questions_answers = load_knowledge_source(knowledge_source_path)\n",
        "\n",
        "    # Embed and index the questions\n",
        "    context_encoder, context_tokenizer, index = embed_and_index_knowledge(questions_answers)\n",
        "\n",
        "    # Example query\n",
        "    query = \"help me. what is Trichiasis?\"\n",
        "\n",
        "    # Retrieve information for the query\n",
        "    retrieved_data, distances = retrieve_top_k(\n",
        "        question_encoder,\n",
        "        question_tokenizer,\n",
        "        index,\n",
        "        query,\n",
        "        questions_answers\n",
        "    )\n",
        "\n",
        "    # Generate an answer using the retrieved data\n",
        "    answer = generate_answer_with_model(query, retrieved_data[:2])  # Use top 2 retrieved results\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"\\nGenerated Answer:\\n{answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "26345e5345014a30ad34c0e8579fdf6e"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-25T13:26:34.227373Z",
          "iopub.status.busy": "2024-12-25T13:26:34.227021Z",
          "iopub.status.idle": "2024-12-25T13:27:16.051624Z",
          "shell.execute_reply": "2024-12-25T13:27:16.050844Z",
          "shell.execute_reply.started": "2024-12-25T13:26:34.227331Z"
        },
        "id": "KA-7isTzFjSL",
        "outputId": "84650692-806d-47ca-d89b-f908fcc8b36c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26345e5345014a30ad34c0e8579fdf6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: What are some of the common medications for Myoclonus?\n",
            "\n",
            "Generated Answer:\n",
            "**The following medications are commonly used to treat myoclonus:**\n",
            "\n",
            "* **Clonazepam**\n",
            "* **Levetiracetam**\n",
            "* **Lamotrigine**\n",
            "* **Oxazepam**\n",
            "* **Gabapentin** \n",
            "* **Ethosuximide** \n",
            "\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "Myoclonus is a sudden, involuntary muscle contraction.  These medications are generally used to treat various types of epilepsy, but they can also be effective in managing myoclonus.\n",
            "\n",
            "\n",
            "**Please note:** This is not an exhaustive list, and the best treatment for myoclonus will vary depending on the underlying cause and individual patient factors. Always consult with a healthcare professional for diagnosis and treatment.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import (\n",
        "    DPRContextEncoder,\n",
        "    DPRQuestionEncoder,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Load the knowledge source\n",
        "def load_knowledge_source(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    questions_answers = []\n",
        "    for _, row in df.iterrows():\n",
        "        questions_answers.append({\n",
        "            \"Question\": preprocess_text(row['Question']),  # Apply preprocessing\n",
        "            \"Answer\": row['Answer']\n",
        "        })\n",
        "    return questions_answers\n",
        "\n",
        "# Embed and index the knowledge source\n",
        "def embed_and_index_knowledge(questions_answers):\n",
        "    context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    embeddings = []\n",
        "    max_length = 256\n",
        "\n",
        "    for qa in questions_answers:\n",
        "        text = preprocess_text(f\"Question: {qa['Question']}\")  # Only include the question\n",
        "        inputs = context_tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = context_encoder(**inputs).pooler_output\n",
        "            normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
        "            embeddings.append(normalized_embedding.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)  # Ensure Inner Product similarity\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return context_encoder, context_tokenizer, index\n",
        "\n",
        "# Retrieve the top-k results\n",
        "def retrieve_top_k(question_encoder, question_tokenizer, index, query, questions_answers, k=2):\n",
        "    query = preprocess_text(query)  # Preprocess query\n",
        "\n",
        "    # Tokenize the query\n",
        "    inputs = question_tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "        normalized_query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1).cpu().numpy()\n",
        "\n",
        "    # Retrieve the top k most relevant questions\n",
        "    distances, indices = index.search(normalized_query_embedding, k)\n",
        "\n",
        "    # Get the corresponding questions and answers\n",
        "    retrieved_data = [\n",
        "        {\"Question\": questions_answers[i]['Question'], \"Answer\": questions_answers[i]['Answer']}\n",
        "        for i in indices[0]\n",
        "    ]\n",
        "    return retrieved_data, distances[0]\n",
        "\n",
        "# Generate an answer using a language model\n",
        "def generate_answer_with_model(question, retrieved_contexts):\n",
        "    \"\"\"\n",
        "    Generate an answer based on the retrieved contexts and the question.\n",
        "\n",
        "    Args:\n",
        "        question (str): The original query.\n",
        "        retrieved_contexts (list): Retrieved documents to use for answering the question.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer.\n",
        "    \"\"\"\n",
        "    # Initialize the language model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token='')\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", token='').to(device)\n",
        "\n",
        "    # Construct the input prompt with retrieved contexts\n",
        "    prompt = f\"\"\"\n",
        "Below are some relevant documents that might help answer the question.\n",
        "If the answer can be found in these documents, please provide it.\n",
        "\n",
        "\n",
        "Documents:\n",
        "{retrieved_contexts[0]['Answer']}\n",
        "{retrieved_contexts[1]['Answer']}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Tokenize the input prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    # Generate the answer\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids[\"input_ids\"],\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.5,  # Lower temperature for more deterministic output\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # Decode the output\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the generated answer (after \"Answer:\")\n",
        "    answer_parts = generated_text.split(\"Answer:\")\n",
        "    if len(answer_parts) > 1:\n",
        "        return answer_parts[-1].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load the question encoder model and tokenizer\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(\n",
        "        \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "    ).to(device)\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
        "        \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "    )\n",
        "\n",
        "    # Load the knowledge source (CSV with question-answer pairs)\n",
        "    knowledge_source_path = \"knowledge-source.csv\"\n",
        "    questions_answers = load_knowledge_source(knowledge_source_path)\n",
        "\n",
        "    # Embed and index the questions\n",
        "    context_encoder, context_tokenizer, index = embed_and_index_knowledge(questions_answers)\n",
        "\n",
        "    # Example query\n",
        "    query = \"What are some of the common medications for Myoclonus?\"\n",
        "\n",
        "    # Retrieve information for the query\n",
        "    retrieved_data, distances = retrieve_top_k(\n",
        "        question_encoder,\n",
        "        question_tokenizer,\n",
        "        index,\n",
        "        query,\n",
        "        questions_answers\n",
        "    )\n",
        "\n",
        "    # Generate an answer using the retrieved data\n",
        "    answer = generate_answer_with_model(query, retrieved_data[:1])  # Use top 2 retrieved results\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"\\nGenerated Answer:\\n{answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d72c6e4108384738874a702a6f38d2a3"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-25T13:38:05.701791Z",
          "iopub.status.busy": "2024-12-25T13:38:05.701462Z",
          "iopub.status.idle": "2024-12-25T13:38:46.922599Z",
          "shell.execute_reply": "2024-12-25T13:38:46.921844Z",
          "shell.execute_reply.started": "2024-12-25T13:38:05.701765Z"
        },
        "id": "nho4jrIoFjSL",
        "outputId": "eb8460ee-71cd-43f0-dcb6-9c03b6fed371",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d72c6e4108384738874a702a6f38d2a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Disease: Aphakia\n",
            "Symptoms of Aphakia: Diminished vision, Symptoms of eye, Pain in eye, Eye redness, Itchiness of eye, Spots or clouds in vision, Eye burns or stings, Mass on eyelid\n",
            "Answer: **John is more likely to have Aphakia.**\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "Aphakia is a condition where the natural lens of the eye is absent or has become significantly impaired. The symptoms you've described are consistent with this condition. \n",
            "\n",
            "* **John's symptoms:** Pain in the eye, diminished vision, and spots or clouds in vision all point to a problem with the eye's structure or function.\n",
            "* **Sofia's symptoms:** While her symptoms are concerning, they are more associated with issues like eye infections, inflammation, or other eye conditions.\n",
            "\n",
            "\n",
            "**Important Note:**  This is an educated guess based on the provided information. It is crucial to remember that **diagnosing medical conditions requires a professional medical\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import (\n",
        "    DPRContextEncoder,\n",
        "    DPRQuestionEncoder,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Load the knowledge source\n",
        "def load_knowledge_source(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    questions_answers = []\n",
        "    for _, row in df.iterrows():\n",
        "        questions_answers.append({\n",
        "            \"Question\": preprocess_text(row['Question']),\n",
        "            \"Answer\": row['Answer']\n",
        "        })\n",
        "    return questions_answers\n",
        "\n",
        "# Embed and index the knowledge source\n",
        "def embed_and_index_knowledge(questions_answers):\n",
        "    context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    embeddings = []\n",
        "    max_length = 256\n",
        "\n",
        "    for qa in questions_answers:\n",
        "        text = preprocess_text(f\"Question: {qa['Question']}\")\n",
        "        inputs = context_tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = context_encoder(**inputs).pooler_output\n",
        "            normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
        "            embeddings.append(normalized_embedding.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return context_encoder, context_tokenizer, index\n",
        "\n",
        "# Retrieve the top-k results\n",
        "def retrieve_top_k(question_encoder, question_tokenizer, index, query, questions_answers, k=2):\n",
        "    query = preprocess_text(query)\n",
        "\n",
        "    inputs = question_tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "        normalized_query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1).cpu().numpy()\n",
        "\n",
        "    distances, indices = index.search(normalized_query_embedding, k)\n",
        "\n",
        "    retrieved_data = [\n",
        "        {\"Question\": questions_answers[i]['Question'], \"Answer\": questions_answers[i]['Answer']}\n",
        "        for i in indices[0]\n",
        "    ]\n",
        "    return retrieved_data, distances[0]\n",
        "\n",
        "# Extract the disease from the query\n",
        "def extract_disease_from_query(query):\n",
        "    pattern = r\"which person is more likely to have (\\w+)\\??\"\n",
        "    match = re.search(pattern, query, re.IGNORECASE)\n",
        "    return match.group(1).strip() if match else None\n",
        "\n",
        "# Generate sub-query for disease symptoms\n",
        "def generate_disease_symptoms_query(disease):\n",
        "    return f\"What are the symptoms of {disease}\"\n",
        "\n",
        "# Generate answer using the model\n",
        "def generate_answer_with_model(main_query, symptoms_context):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token='')\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", token='').to(device)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Below is the relevant context to answer the question.\n",
        "\n",
        "Retrieved Context:\n",
        "{symptoms_context}\n",
        "\n",
        "Question: {main_query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids[\"input_ids\"],\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer_parts = generated_text.split(\"Answer:\")\n",
        "    if len(answer_parts) > 1:\n",
        "        return answer_parts[-1].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "    knowledge_source_path = \"knowledge-source.csv\"\n",
        "    questions_answers = load_knowledge_source(knowledge_source_path)\n",
        "    context_encoder, context_tokenizer, index = embed_and_index_knowledge(questions_answers)\n",
        "\n",
        "    query = \"Sofia has symptoms such as focal weakness, vaginal pain and pelvic pain. John has symptoms such as pain in eye, diminished vision and spots in vision. Which person is more likely to have Aphakia?\"\n",
        "\n",
        "    disease = extract_disease_from_query(query)\n",
        "    if not disease:\n",
        "        print(\"No disease found in query.\")\n",
        "        return\n",
        "\n",
        "    disease_query = generate_disease_symptoms_query(disease)\n",
        "    retrieved_data, _ = retrieve_top_k(question_encoder, question_tokenizer, index, disease_query, questions_answers)\n",
        "\n",
        "    if not retrieved_data:\n",
        "        print(f\"No information found for disease: {disease}\")\n",
        "        return\n",
        "\n",
        "    symptoms_context = retrieved_data[0]['Answer']\n",
        "    answer = generate_answer_with_model(query, symptoms_context)\n",
        "\n",
        "    print(f\"Disease: {disease}\")\n",
        "    print(f\"Symptoms of {disease}: {symptoms_context}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5f867125e4d64cb192495abba24c06fe"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-25T13:47:44.426067Z",
          "iopub.status.busy": "2024-12-25T13:47:44.425771Z",
          "iopub.status.idle": "2024-12-25T13:48:23.331066Z",
          "shell.execute_reply": "2024-12-25T13:48:23.330170Z",
          "shell.execute_reply.started": "2024-12-25T13:47:44.426045Z"
        },
        "id": "vkiF1bORFjSM",
        "outputId": "5e9d842d-c5af-44cd-ab27-7c3e45c2cd34",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f867125e4d64cb192495abba24c06fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Disease: Juvenile rheumatoid arthritis\n",
            "Symptoms of Juvenile rheumatoid arthritis: Juvenile idiopathic arthritis (JIA), also known as juvenile rheumatoid arthritis (JRA), is the most common form of arthritis in children and adolescents. (Juvenile in this context refers to an onset before age 16, idiopathic refers to a condition with no defined cause, and arthritis is the inflammation of the synovium of a joint.) \n",
            "Answer: **Charlotte** is more likely to have Juvenile rheumatoid arthritis. \n",
            "\n",
            "**Reasoning:**\n",
            "\n",
            "* **Charlotte's symptoms:** The symptoms Charlotte experiences (knee pain, back pain, ankle pain, finger swelling, wrist swelling, and problems with movement) are common in Juvenile idiopathic arthritis (JIA).\n",
            "* **Sally's symptoms:** Sally's symptoms (vomiting, feeling ill, fever, fluid retention, headache, and fainting) are more consistent with systemic illnesses or infections\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import (\n",
        "    DPRContextEncoder,\n",
        "    DPRQuestionEncoder,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "# Load the knowledge source\n",
        "def load_knowledge_source(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    questions_answers = []\n",
        "    for _, row in df.iterrows():\n",
        "        questions_answers.append({\n",
        "            \"Question\": preprocess_text(row['Question']),\n",
        "            \"Answer\": row['Answer']\n",
        "        })\n",
        "    return questions_answers\n",
        "\n",
        "# Embed and index the knowledge source\n",
        "def embed_and_index_knowledge(questions_answers):\n",
        "    context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "    embeddings = []\n",
        "    max_length = 256\n",
        "\n",
        "    for qa in questions_answers:\n",
        "        text = preprocess_text(f\"Question: {qa['Question']}\")\n",
        "        inputs = context_tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = context_encoder(**inputs).pooler_output\n",
        "            normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
        "            embeddings.append(normalized_embedding.cpu())\n",
        "\n",
        "    embeddings = torch.cat(embeddings).numpy()\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    return context_encoder, context_tokenizer, index\n",
        "\n",
        "# Retrieve the top-k results\n",
        "def retrieve_top_k(question_encoder, question_tokenizer, index, query, questions_answers, k=2):\n",
        "    query = preprocess_text(query)\n",
        "\n",
        "    inputs = question_tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = question_encoder(**inputs).pooler_output\n",
        "        normalized_query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1).cpu().numpy()\n",
        "\n",
        "    distances, indices = index.search(normalized_query_embedding, k)\n",
        "\n",
        "    retrieved_data = [\n",
        "        {\"Question\": questions_answers[i]['Question'], \"Answer\": questions_answers[i]['Answer']}\n",
        "        for i in indices[0]\n",
        "    ]\n",
        "    return retrieved_data, distances[0]\n",
        "\n",
        "# Extract the disease from the query\n",
        "def extract_disease_from_query(query):\n",
        "    pattern = r\"which person is more likely to have ([\\w\\s]+)\\??\"\n",
        "    match = re.search(pattern, query, re.IGNORECASE)\n",
        "    return match.group(1).strip() if match else None\n",
        "\n",
        "# Generate sub-query for disease symptoms\n",
        "def generate_disease_symptoms_query(disease):\n",
        "    return f\"What are the symptoms of {disease}\"\n",
        "\n",
        "# Generate answer using the model\n",
        "def generate_answer_with_model(main_query, symptoms_context):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token='')\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", token='').to(device)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Below is the relevant context to answer the question.\n",
        "\n",
        "Retrieved Context:\n",
        "{symptoms_context}\n",
        "\n",
        "Question: {main_query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids[\"input_ids\"],\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer_parts = generated_text.split(\"Answer:\")\n",
        "    if len(answer_parts) > 1:\n",
        "        return answer_parts[-1].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "    knowledge_source_path = \"knowledge-source.csv\"\n",
        "    questions_answers = load_knowledge_source(knowledge_source_path)\n",
        "    context_encoder, context_tokenizer, index = embed_and_index_knowledge(questions_answers)\n",
        "\n",
        "    query = \"Charlotte is a judge and has symptoms such as Knee pain, back pain, ankle pain, finger swelling, wrist swelling and problems with movement. Sally is unemployed and has symptoms such as vomiting, feeling ill, fever, fluid retention, headache and fainting. Which person is more likely to have Juvenile rheumatoid arthritis?\"\n",
        "\n",
        "    disease = extract_disease_from_query(query)\n",
        "    if not disease:\n",
        "        print(\"No disease found in query.\")\n",
        "        return\n",
        "\n",
        "    disease_query = generate_disease_symptoms_query(disease)\n",
        "    retrieved_data, _ = retrieve_top_k(question_encoder, question_tokenizer, index, disease_query, questions_answers)\n",
        "\n",
        "    if not retrieved_data:\n",
        "        print(f\"No information found for disease: {disease}\")\n",
        "        return\n",
        "\n",
        "    symptoms_context = retrieved_data[0]['Answer']\n",
        "    answer = generate_answer_with_model(query, symptoms_context)\n",
        "\n",
        "    print(f\"Disease: {disease}\")\n",
        "    print(f\"Symptoms of {disease}: {symptoms_context}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6371632,
          "sourceId": 10294850,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
